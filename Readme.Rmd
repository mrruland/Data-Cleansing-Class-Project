---
title: "README"
author: "Mike Ruland"
date: "Monday, March 16, 2015"
output: html_document
---

This README file is an R Markdown document which explains how to run the 
*run_analysis.R* program used to deliver the class project requirements.

The following libraries are used in my solution:
  **plyr**
  **dplyr**
  **data.table**
  **reshape**
  **lubridate**

There are a set of control variables at the beginning of the script 
to assist with following the flow, displaying intermediate results and naming
the directory in the event the data needs to be downloaded.  If you already 
have the data downloaded and unzipped in the current working directory, simply 
change the value of the *"unzippedfiles"* variable to point to the directory 
containing the data.  The program will look in the working directory for the 
sub-directory specified in this variable to determine if the data exists.  

There is a control variable called *viewtables* which will use the **View()**
function to nicely format the 3 main intermediate tables to provide an easy
way to see the intermediate steps prior to writing out the data (note that 
View() seems to work best using **R-Studio**).  The program is also coded to 
produce either a wide tidy dataset and/or a long tidy data set based on the 
control variables *wide* and *long*.  By default the script looks for the 
unzipped file in the directory **"./ProjectFileUnzipped"** and produces a 
**wide** tidy dataset.

The script loads the file names from the directory stucture in the unzipped 
download file that looks like this (directories are bold format):
   **working directory**
     **UCI HAR Dataset**
        **test**
          **Initial Sample**
          subject_test.txt          <-- subject id for test data set
          X_test.txt                <-- measurements for test data set
          Y_test.txt                <-- activity for test data set
        **train**
          **Initial Sample**
          subject_train.txt         <-- subject id for train data set
          X_train.txt               <-- measurements for train data set
          Y_train.txt               <-- activity for train data set
        activity_labels.txt         <-- names of the activities monitored
        features.txt                <-- variable names in the study
        features_info.txt
        README.txt
        
There are 2 functions created in the script *"getdata()"* downloads and unzips
the source data file if necessary.  *"cleanvariablenames()"* is used to clean 
the variable names - I chose a function to implement this to make it easy to 
reuse across multiple projects once a standard is set.

The script looks for the directory containing the data and creates it if does 
not exist.  Next it parses the directories and loads the name of the txt files 
to read in into arrays which are then used during the read operation.  I chose 
this approach while reading the data in the *Initial Sample* directory since 
there were a large number of files - I later found they were unnecessary for 
the solution to the exercise.  Once read in, the data from the test files 
are merged together using *cbind()* function and then train files are merged
in a similar fashion - allowing them to align properly.  Finally the test and 
train datasets are concatonated using an *rbind()* function.

At this point I only retain variables containing the string *mean* and *std*
based on the instructions in the project.  I chose not to keep the variables
containing the string *Mean* (capital *M*) since these variables were mentioned
in the forum as likely not being needed - note there there is a comment in the 
code where the value *keepcolumns* is derived discussing the minor change 
needed to include these values in the result.  About this same point I also 
obtain the variable names associated with the retained columns (as well as the 
subject and activity columns).  

Next the variable names are *"standardized"* using the *cleanvariablenames()*
function.  I then use pipelining to create the *widegroupmeans* dataset and the 
*longgroupmeans* dataset, if specified in the contol variables.  

Finally the requested dataset(s) is writen out using the **write.table()** 
function and a message is written to the user explaining how to read the tables
back into **R**.

To run the script, simply place the *run_analysis.R* script in your working 
directory, check that the values in the control varibles match your needs - 
especially the name of the directory containing the unzipped files - and 
issue the command **source("run_analysis.R")**.  This will run the script 
and produce the tidy dataset chosen in your working directory.